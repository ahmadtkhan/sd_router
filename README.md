# Software Defined Router
This project utilizes Rust's memory safe, high performance, and safe concurrency features to build a Software Defined Router which can completely run on your local machine.
Motivation: Although there is a lot of support for Sofware Defined networking in forms of crates and examples, there are not a lot of applications for networking in Rust. Secondly, routing complexity and the surface area for security flaws both scale with network size is pushing us to rethink how we construct data and control planes. Rust’s ownership model and zero-cost abstractions help us avoid buffer overflows, dangling pointers, and data races that have historically plagued packet processors and protocols written in unsafe languages.

## Creating a network
The unrouted network is defined by the linux kernel as a user defined number of routers and hosts. It uses Linux network namespaces, veth pairs, and either a Linux bridge or Open vSwitch to create a random number of hosts and routers on a single box. It then shapes links with tc netem and replay traffic with PCAPs. There is no hardware required in any step. Conceptually, the router is split into a fast, pluggable data plane and a flexible control plane.

## Router overview
The program has two modes, Kernel-assist and fast-mode In its kernel-assist mode, the router programs the Linux FIB via netlink and delegates forwarding to the kernel, using nftables/TC for ACLs and QoS—ideal for the first milestone because it’s simple and stable. In its programmbale fast-mode an XDF program that parses L2/L3 header, does longest-prefix match via BPF maps, applies stateless ACLs, decrements TTL, and redirects packets while the user selects ARP/NDP and neighbor liveness. Both modes have the same features as they support IPv4, and IPv6, with deterministic hashing, ICMP error generation and other standard primitives. 

Implementation of the program is done in phases: Phase 0 boots a kernel-backed forwarder (fast MVP, easy debugging); Phase 1 adds policy/ECMP and a proper prefixs; Phase 2 introduces neighbors/BFD; Phase 3 swaps in the XDP data plane for line-rate performance; Phase 4 integrates BGP/OSPF via a sidecar (FRRouting or GoBGP) with a clean migration path to native Rust agents; Phase 5 layers tunnels (VXLAN, SRv6) and QoS, plus INT/IOAM for per-hop latency insight. The router’s inputs are cleanly modeled: a YAML/TOML config describes topology, interfaces (NIC/TUN/TAP/XDP), IP addressing, VRFs, policies, ACLs, and optional tunnels; dynamic routes and intents arrive over gRPC; telemetry returns via metrics and streams; test inputs include PCAPs and impairment scripts for repeatable experiments.

To keep the code lean and reliable multiple crates are used: tokio and async-trait for concurrency; serde serde_yaml/toml, and config for configuration; tracing and metrics with the Prometheus exporter for observability; rtnetlink (and friends) for kernel programming; etherparse (or pnet_packet) for header handling in user space; aya/aya-bpf for eBPF/XDP pipelines; ipnet, cidr, and ip_network_table or a radix trie for fast LPM; tonic and prost for the gRPC API; thiserror/anyhow for error hygiene; and pcap/pcap-file for offline replay and debugging. The result is a versatile, router that can scale from a single laptop lab to many containerized nodes orchestrated by containerlab or Kubernetes; it offers a stable northbound API so future AI components can propose policy deltas without touching the hot path, while Rust’s safety guarantees and structured concurrency keep the system robust under churn and fault conditions.

